{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazal735/DL_A2/blob/main/DL_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH4UR8swGpVy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyAQJsl3nQpL",
        "outputId": "947fa0c6-8c23-4135-82f2-ea91247316d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/drive/MyDrive/inaturalist_12K/train'\n",
        "test_data_dir = '/content/drive/MyDrive/inaturalist_12K/val'"
      ],
      "metadata": {
        "id": "38ChQLPx-Kta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9B_ez4BtbKJ"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7pou-cN_xvq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train data loading\n",
        "def train_data(train_data_dir,data_augmentation):\n",
        "  size=transforms.Resize((224,224))\n",
        "  to_tensor=transforms.ToTensor()\n",
        "  #check again-autogenerated\n",
        "  normalize=transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "  crop=transforms.RandomResizedCrop(224)\n",
        "  flip=transforms.RandomHorizontalFlip()\n",
        "  #try changing the values\n",
        "  color=transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "  rotation=transforms.RandomRotation(30)\n",
        "\n",
        "\n",
        "  #augmentation of image\n",
        "  if data_augmentation == 'Yes':\n",
        "        transform_img = transforms.Compose([crop,flip,color, rotation, to_tensor,normalize]) # Data transformations\n",
        "\n",
        "  else:\n",
        "      transform_img = transforms.Compose([size,to_tensor, normalize ])\n",
        "\n",
        "\n",
        "\n",
        "  #data fetchiing\n",
        "  training_data=torchvision.datasets.ImageFolder(train_data_dir, transform=transform_img)\n",
        "\n",
        "\n",
        "  train_index, val_index = train_test_split(list(range(len(training_data))), test_size=0.2, random_state=42)\n",
        "  train_sampler = SubsetRandomSampler(train_index)\n",
        "  val_sampler = SubsetRandomSampler(val_index)\n",
        "\n",
        "  train_data=DataLoader(training_data,batch_size=32,sampler=train_sampler)\n",
        "  validation_data = DataLoader(training_data, batch_size=32, sampler=val_sampler)\n",
        "  print('Train data size:', len(train_data))\n",
        "  print('Validation data size:', len(validation_data))\n",
        "\n",
        "  return train_data,validation_data\n",
        "\n",
        "#test data loading\n",
        "def test_data(test_data_dir,data_augmentation):\n",
        "  size=transforms.Resize((224,224))\n",
        "  to_tensor=transforms.ToTensor()\n",
        "  #check again-autogenerated\n",
        "  normalize=transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "\n",
        "\n",
        "  #augmentation of image\n",
        "  data_transform=transforms.Compose([size,to_tensor,normalize])\n",
        "\n",
        "  #data fetching\n",
        "  test_data=torchvision.datasets.ImageFolder(test_data_dir,transform=data_transform)\n",
        "  test_data=DataLoader(test_data,batch_size=32)\n",
        "\n",
        "\n",
        "  return test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJKcqO-0Ku_"
      },
      "source": [
        "model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn01KxwgGnYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75437f9-f1e0-45ee-e200-38632f6fa06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model(\n",
            "  (activation): ReLU()\n",
            "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout2): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout3): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout4): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer5): Conv2d(256, 512, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout5): Dropout2d(p=0, inplace=False)\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout_layer): Dropout1d(p=0, inplace=False)\n",
            "  (fc): Linear(in_features=8192, out_features=512, bias=True)\n",
            "  (fc_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (output_layer): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self,num_filters=[32,64,128,256,512],filter_size=[3,3,5,5,7],activation=nn.ReLU(),\n",
        "               stride=1, padding=1, pool_size=(2,2),fc_size=512,nom_o_classes=10,\n",
        "               dropout=0,in_channels=3,batch_norm='YES'):\n",
        "    super(model,self).__init__()\n",
        "    self.num_filters=num_filters\n",
        "    self.filter_size=filter_size\n",
        "    self.activation=activation\n",
        "    self.stride=stride\n",
        "    self.padding=padding\n",
        "    self.pool_size=pool_size\n",
        "    self.fc_size=fc_size\n",
        "    self.nom_o_classes=nom_o_classes\n",
        "    self.dropout=dropout\n",
        "    self.channels=in_channels\n",
        "\n",
        "\n",
        "    def image_size(img_w,filter_size,padding,stride):\n",
        "      return ((img_w-filter_size+2*padding)/stride+1)*0.5\n",
        "\n",
        "\n",
        "    #layers of convolution\n",
        "    #layer1\n",
        "    self.conv_layer1=nn.Conv2d(self.channels,self.num_filters[0], stride=self.stride, padding=self.padding,\n",
        "                               kernel_size=self.filter_size[0])\n",
        "    self.batch_norm1=nn.BatchNorm2d(self.num_filters[0])\n",
        "    self.dropout1=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size1=image_size(224,self.filter_size[0],self.padding,self.stride)\n",
        "\n",
        "    #layer2\n",
        "    self.conv_layer2=nn.Conv2d(self.num_filters[0],self.num_filters[1], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[1])\n",
        "    self.batch_norm2=nn.BatchNorm2d(self.num_filters[1])\n",
        "    self.dropout2=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size2=image_size(img_size1,self.filter_size[1],self.padding,self.stride)\n",
        "\n",
        "    #layer3\n",
        "    self.conv_layer3=nn.Conv2d(self.num_filters[1],self.num_filters[2], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[2])\n",
        "    self.batch_norm3=nn.BatchNorm2d(self.num_filters[2])\n",
        "    self.dropout3=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size3=image_size(img_size2,self.filter_size[2],self.padding,self.stride)\n",
        "\n",
        "    #layer4\n",
        "    self.conv_layer4=nn.Conv2d(self.num_filters[2],self.num_filters[3], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[3])\n",
        "    self.batch_norm4=nn.BatchNorm2d(self.num_filters[3])\n",
        "    self.dropout4=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size4=image_size(img_size3,self.filter_size[3],self.padding,self.stride)\n",
        "\n",
        "    #layer5\n",
        "    self.conv_layer5=nn.Conv2d(self.num_filters[3],self.num_filters[4], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[4])\n",
        "    self.batch_norm5=nn.BatchNorm2d(self.num_filters[4])\n",
        "    self.dropout5=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size5=int(image_size(img_size4,self.filter_size[4],self.padding,self.stride))\n",
        "\n",
        "\n",
        "    self.pool=nn.MaxPool2d(self.pool_size,stride=2)\n",
        "\n",
        "    self.dropout_layer = nn.Dropout1d(self.dropout)\n",
        "\n",
        "    # Define fully connected layer\n",
        "    self.fc = nn.Linear(self.num_filters[4] * (img_size5 ** 2), self.fc_size)\n",
        "    self.fc_bn = nn.BatchNorm1d(self.fc_size)  # Batch normalization for fully connected layer\n",
        "\n",
        "    # Output layer\n",
        "    self.output_layer = nn.Linear(self.fc_size, self.nom_o_classes)\n",
        "\n",
        "    # forward\n",
        "  def forward(self,x):\n",
        "    #layer1\n",
        "    x=self.conv_layer1(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout1(x)\n",
        "    x=self.batch_norm1(x)\n",
        "\n",
        "      #layer2\n",
        "    x=self.conv_layer2(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.batch_norm2(x)\n",
        "\n",
        "    #layer3\n",
        "    x=self.conv_layer3(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout3(x)\n",
        "    x=self.batch_norm3(x)\n",
        "\n",
        "    #layer4\n",
        "    x=self.conv_layer4(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout4(x)\n",
        "    x=self.batch_norm4(x)\n",
        "\n",
        "    #layer5\n",
        "    x=self.conv_layer5(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout5(x)\n",
        "    x=self.batch_norm5(x)\n",
        "\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "    x = self.fc_bn(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.dropout_layer(x)\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model1=model()\n",
        "model1.to(device)\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOrkblSLG7sz"
      },
      "outputs": [],
      "source": [
        "epochs=100\n",
        "learning_rate=0.1\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAJDHetA0Akv"
      },
      "source": [
        "model training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSNnQI40HJI0"
      },
      "outputs": [],
      "source": [
        "# loss_metric=nn.CrossEntropyLoss()\n",
        "# optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)\n",
        "\n",
        "# def training(model1,data):\n",
        "\n",
        "#   model1.train(True)\n",
        "#   training_loss=0.0\n",
        "#   true_label=0\n",
        "#   total_train=0\n",
        "\n",
        "#   for input, label in data:\n",
        "#     input = input.to(device)\n",
        "#     label = label.to(device)\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     output=model1(input)\n",
        "#     loss=loss_metric(output,label)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     training_loss += loss.item()\n",
        "#     _,predicted=torch.max(output.data,1)\n",
        "#     total_train += label.size(0)\n",
        "#     true_label += (predicted==label).sum().item()\n",
        "\n",
        "#   train_accuracy=100*true_label/total_train\n",
        "#   return train_accuracy,training_loss,model1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2agnKN30E54"
      },
      "source": [
        "model testing function(on validation data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_metric=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)\n",
        "\n",
        "def training(model1, data, epoch=0):\n",
        "    model1.train(True)\n",
        "    training_loss = 0.0\n",
        "    true_label = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Add batch tracking\n",
        "    total_batches = len(data)\n",
        "\n",
        "    for batch_idx, (input, label) in enumerate(data):\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model1(input)\n",
        "        loss = loss_metric(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_train += label.size(0)\n",
        "        true_label += (predicted == label).sum().item()\n",
        "\n",
        "        # Print progress every few batches\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
        "            print(f'Epoch: {epoch} [{batch_idx+1}/{total_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "    train_accuracy = 100 * true_label / total_train\n",
        "    avg_loss = training_loss / total_batches\n",
        "\n",
        "    print(f\"Training completed - Accuracy: {train_accuracy:.2f}%, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return train_accuracy, training_loss, model1"
      ],
      "metadata": {
        "id": "97LWLz1CPXwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtXvXgFRx4o5"
      },
      "outputs": [],
      "source": [
        "def test_on_valid_data(model, test_data):\n",
        "    model.eval()\n",
        "\n",
        "    correct_label = 0\n",
        "    total_label = 0\n",
        "    with torch.no_grad():\n",
        "        for img, label in test_data:\n",
        "            img, label = img.to(device), label.to(device)\n",
        "            output = model(img)\n",
        "\n",
        "            _, pred = torch.max(output, 1)\n",
        "            correct_label += (pred == label).sum().item()\n",
        "            total_label += label.size(0)\n",
        "\n",
        "    valid_accuracy = 100 * correct_label / total_label\n",
        "    print(f'Validation Accuracy: {valid_accuracy:.2f}%')\n",
        "    return valid_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnUsHUP-6Aru"
      },
      "outputs": [],
      "source": [
        "def model_train_val(model, train_data, val_data,epochs,device=device):\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    print(\"model train val\")\n",
        "    for epoch in range(epochs):\n",
        "        train_accuracy,avg_loss,model1 = training(model, train_data)\n",
        "        # Print training loss and accuracy\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "        # Validation loop\n",
        "        val_accuracy = test_on_valid_data(model, val_data)\n",
        "        # Print validation accuracy\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "        wandb.log({'Train loss': avg_loss})\n",
        "        wandb.log({'Train accuracy': train_accuracy})\n",
        "\n",
        "        wandb.log({'val_accuracy': val_accuracy})\n",
        "        wandb.log({'epoch': epoch})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def model_train_val(model, train_data, val_data, epochs, device=device):\n",
        "    print(\"Starting model_train_val function\")\n",
        "\n",
        "    # Define criterion and optimizer here\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Pass these to your training function if needed\n",
        "    # Or ensure global variables are properly synchronized\n",
        "\n",
        "    print(f\"Beginning training for {epochs} epochs\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Capture start time to monitor duration\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            print(\"Calling training function...\")\n",
        "            train_accuracy, avg_loss, model = training(model, train_data)\n",
        "            print(f\"Training complete for epoch {epoch+1}\")\n",
        "\n",
        "            # Print training metrics\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "            print(\"Starting validation...\")\n",
        "            val_accuracy = test_on_valid_data(model, val_data)\n",
        "            print(f\"Validation complete for epoch {epoch+1}\")\n",
        "\n",
        "            # Print validation metrics\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "            # Calculate epoch duration\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
        "\n",
        "            # Log to wandb with try-except to catch any logging errors\n",
        "            try:\n",
        "                print(\"Logging to wandb...\")\n",
        "                wandb.log({\n",
        "                    'epoch': epoch,\n",
        "                    'train_loss': avg_loss,\n",
        "                    'train_accuracy': train_accuracy,\n",
        "                    'val_accuracy': val_accuracy,\n",
        "                    'epoch_time': epoch_time\n",
        "                })\n",
        "                print(\"Successfully logged to wandb\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error logging to wandb: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during epoch {epoch+1}: {str(e)}\")\n",
        "            traceback.print_exc()  # This will print the full stack trace\n",
        "\n",
        "    print(\"Model training and validation completed\")\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xzowtcjtKQZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7rR72xABts",
        "outputId": "bee959bf-0998-4250-badd-0f7094c988ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e4d0a8c3ccaf2534e9ab91c659e420ba5114533f\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "BKO6dIE-q2Oy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "OlwujivySuZk",
        "outputId": "5cf86737-8c8f-4875-b509-3bbcfbe879a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmfazal735\u001b[0m (\u001b[33mmfazal735-iit-madras-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwKIfz6h8Gx1",
        "outputId": "ac852556-093f-40e1-b713-8bc454eac824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 65pmdmkr\n",
            "Sweep URL: https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'kernel_size':{\n",
        "            'values': [[3,3,3,3,3],[3,5,5,7,7],[3,5,3,5,7],[5,5,5,5,5]]#,[7,7,7,7,7]]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.3, 0.2]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': [ 'relu','mish','silu', 'gelu',]\n",
        "        },\n",
        "        'num_dense':{\n",
        "            'values': [128, 256]\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': ['Yes','No']\n",
        "        },\n",
        "        'filter_org':{\n",
        "            'values': [[128,128,64,64,32],[32,64,128,256,512],[32,32,32,32,32],[32,64,64,128,128]]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values': [0.001,0.0001]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': ['Adam','SGD']\n",
        "        },\n",
        "        'data_aug': {\n",
        "            'values': ['No', 'Yes']\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL A2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5AoQOnSN9LyE",
        "outputId": "3bbad64c-ed85-4cf2-e80b-baa90f9e8eec"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3z4tehjr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: Yes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: [32, 32, 32, 32, 32]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 5, 5, 7, 7]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dense: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DL A2' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250416_083243-3z4tehjr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/3z4tehjr' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/3z4tehjr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/3z4tehjr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 250\n",
            "Validation data size: 63\n",
            "Starting model_train_val function\n",
            "Beginning training for 10 epochs\n",
            "Starting epoch 1/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3019\n",
            "Epoch: 0 [20/250] Loss: 2.2779\n",
            "Epoch: 0 [30/250] Loss: 2.4157\n",
            "Epoch: 0 [40/250] Loss: 2.3473\n",
            "Epoch: 0 [50/250] Loss: 2.3740\n",
            "Epoch: 0 [60/250] Loss: 2.3937\n",
            "Epoch: 0 [70/250] Loss: 2.3173\n",
            "Epoch: 0 [80/250] Loss: 2.3553\n",
            "Epoch: 0 [90/250] Loss: 2.2892\n",
            "Epoch: 0 [100/250] Loss: 2.3549\n",
            "Epoch: 0 [110/250] Loss: 2.3009\n",
            "Epoch: 0 [120/250] Loss: 2.3221\n",
            "Epoch: 0 [130/250] Loss: 2.3941\n",
            "Epoch: 0 [140/250] Loss: 2.3710\n",
            "Epoch: 0 [150/250] Loss: 2.2201\n",
            "Epoch: 0 [160/250] Loss: 2.2903\n",
            "Epoch: 0 [170/250] Loss: 2.2598\n",
            "Epoch: 0 [180/250] Loss: 2.2723\n",
            "Epoch: 0 [190/250] Loss: 2.4065\n",
            "Epoch: 0 [200/250] Loss: 2.2486\n",
            "Epoch: 0 [210/250] Loss: 2.2906\n",
            "Epoch: 0 [220/250] Loss: 2.3872\n",
            "Epoch: 0 [230/250] Loss: 2.3618\n",
            "Epoch: 0 [240/250] Loss: 2.2794\n",
            "Epoch: 0 [250/250] Loss: 2.4222\n",
            "Training completed - Accuracy: 10.09%, Avg Loss: 2.3629\n",
            "Training complete for epoch 1\n",
            "Epoch 1/10, Train Loss: 590.7219, Train Accuracy: 10.09%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.05%\n",
            "Validation complete for epoch 1\n",
            "Epoch 1/10, Validation Accuracy: 12.05%\n",
            "Epoch 1 completed in 2802.99 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 2/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3595\n",
            "Epoch: 0 [20/250] Loss: 2.4251\n",
            "Epoch: 0 [30/250] Loss: 2.4340\n",
            "Epoch: 0 [40/250] Loss: 2.3832\n",
            "Epoch: 0 [50/250] Loss: 2.4012\n",
            "Epoch: 0 [60/250] Loss: 2.3089\n",
            "Epoch: 0 [70/250] Loss: 2.4826\n",
            "Epoch: 0 [80/250] Loss: 2.3593\n",
            "Epoch: 0 [90/250] Loss: 2.3800\n",
            "Epoch: 0 [100/250] Loss: 2.4353\n",
            "Epoch: 0 [110/250] Loss: 2.2960\n",
            "Epoch: 0 [120/250] Loss: 2.2565\n",
            "Epoch: 0 [130/250] Loss: 2.2743\n",
            "Epoch: 0 [140/250] Loss: 2.4785\n",
            "Epoch: 0 [150/250] Loss: 2.4491\n",
            "Epoch: 0 [160/250] Loss: 2.2511\n",
            "Epoch: 0 [170/250] Loss: 2.3475\n",
            "Epoch: 0 [180/250] Loss: 2.3199\n",
            "Epoch: 0 [190/250] Loss: 2.4161\n",
            "Epoch: 0 [200/250] Loss: 2.1969\n",
            "Epoch: 0 [210/250] Loss: 2.3776\n",
            "Epoch: 0 [220/250] Loss: 2.3240\n",
            "Epoch: 0 [230/250] Loss: 2.3671\n",
            "Epoch: 0 [240/250] Loss: 2.3830\n",
            "Epoch: 0 [250/250] Loss: 2.3861\n",
            "Training completed - Accuracy: 10.20%, Avg Loss: 2.3628\n",
            "Training complete for epoch 2\n",
            "Epoch 2/10, Train Loss: 590.7003, Train Accuracy: 10.20%\n",
            "Starting validation...\n",
            "Validation Accuracy: 11.80%\n",
            "Validation complete for epoch 2\n",
            "Epoch 2/10, Validation Accuracy: 11.80%\n",
            "Epoch 2 completed in 164.34 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 3/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.4140\n",
            "Epoch: 0 [20/250] Loss: 2.3619\n",
            "Epoch: 0 [30/250] Loss: 2.4268\n",
            "Epoch: 0 [40/250] Loss: 2.3946\n",
            "Epoch: 0 [50/250] Loss: 2.3430\n",
            "Epoch: 0 [60/250] Loss: 2.4288\n",
            "Epoch: 0 [70/250] Loss: 2.3614\n",
            "Epoch: 0 [80/250] Loss: 2.3395\n",
            "Epoch: 0 [90/250] Loss: 2.3338\n",
            "Epoch: 0 [100/250] Loss: 2.4506\n",
            "Epoch: 0 [110/250] Loss: 2.3307\n",
            "Epoch: 0 [120/250] Loss: 2.3642\n",
            "Epoch: 0 [130/250] Loss: 2.3742\n",
            "Epoch: 0 [140/250] Loss: 2.4807\n",
            "Epoch: 0 [150/250] Loss: 2.4615\n",
            "Epoch: 0 [160/250] Loss: 2.3792\n",
            "Epoch: 0 [170/250] Loss: 2.3116\n",
            "Epoch: 0 [180/250] Loss: 2.4256\n",
            "Epoch: 0 [190/250] Loss: 2.3904\n",
            "Epoch: 0 [200/250] Loss: 2.3664\n",
            "Epoch: 0 [210/250] Loss: 2.3610\n",
            "Epoch: 0 [220/250] Loss: 2.3128\n",
            "Epoch: 0 [230/250] Loss: 2.3509\n",
            "Epoch: 0 [240/250] Loss: 2.4085\n",
            "Epoch: 0 [250/250] Loss: 2.3596\n",
            "Training completed - Accuracy: 10.20%, Avg Loss: 2.3659\n",
            "Training complete for epoch 3\n",
            "Epoch 3/10, Train Loss: 591.4656, Train Accuracy: 10.20%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.50%\n",
            "Validation complete for epoch 3\n",
            "Epoch 3/10, Validation Accuracy: 12.50%\n",
            "Epoch 3 completed in 156.11 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 4/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3711\n",
            "Epoch: 0 [20/250] Loss: 2.4114\n",
            "Epoch: 0 [30/250] Loss: 2.3193\n",
            "Epoch: 0 [40/250] Loss: 2.4569\n",
            "Epoch: 0 [50/250] Loss: 2.3804\n",
            "Epoch: 0 [60/250] Loss: 2.3585\n",
            "Epoch: 0 [70/250] Loss: 2.4099\n",
            "Epoch: 0 [80/250] Loss: 2.5048\n",
            "Epoch: 0 [90/250] Loss: 2.4199\n",
            "Epoch: 0 [100/250] Loss: 2.3951\n",
            "Epoch: 0 [110/250] Loss: 2.4721\n",
            "Epoch: 0 [120/250] Loss: 2.4003\n",
            "Epoch: 0 [130/250] Loss: 2.3255\n",
            "Epoch: 0 [140/250] Loss: 2.4336\n",
            "Epoch: 0 [150/250] Loss: 2.4370\n",
            "Epoch: 0 [160/250] Loss: 2.4719\n",
            "Epoch: 0 [170/250] Loss: 2.2927\n",
            "Epoch: 0 [180/250] Loss: 2.1999\n",
            "Epoch: 0 [190/250] Loss: 2.3534\n",
            "Epoch: 0 [200/250] Loss: 2.2376\n",
            "Epoch: 0 [210/250] Loss: 2.4046\n",
            "Epoch: 0 [220/250] Loss: 2.3139\n",
            "Epoch: 0 [230/250] Loss: 2.3402\n",
            "Epoch: 0 [240/250] Loss: 2.3612\n",
            "Epoch: 0 [250/250] Loss: 2.4605\n",
            "Training completed - Accuracy: 10.10%, Avg Loss: 2.3697\n",
            "Training complete for epoch 4\n",
            "Epoch 4/10, Train Loss: 592.4252, Train Accuracy: 10.10%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.85%\n",
            "Validation complete for epoch 4\n",
            "Epoch 4/10, Validation Accuracy: 12.85%\n",
            "Epoch 4 completed in 154.44 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 5/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3722\n",
            "Epoch: 0 [20/250] Loss: 2.3604\n",
            "Epoch: 0 [30/250] Loss: 2.4429\n",
            "Epoch: 0 [40/250] Loss: 2.4965\n",
            "Epoch: 0 [50/250] Loss: 2.3262\n",
            "Epoch: 0 [60/250] Loss: 2.3538\n",
            "Epoch: 0 [70/250] Loss: 2.3286\n",
            "Epoch: 0 [80/250] Loss: 2.3989\n",
            "Epoch: 0 [90/250] Loss: 2.3133\n",
            "Epoch: 0 [100/250] Loss: 2.3769\n",
            "Epoch: 0 [110/250] Loss: 2.3201\n",
            "Epoch: 0 [120/250] Loss: 2.4783\n",
            "Epoch: 0 [130/250] Loss: 2.3359\n",
            "Epoch: 0 [140/250] Loss: 2.3963\n",
            "Epoch: 0 [150/250] Loss: 2.4302\n",
            "Epoch: 0 [160/250] Loss: 2.4428\n",
            "Epoch: 0 [170/250] Loss: 2.3989\n",
            "Epoch: 0 [180/250] Loss: 2.4103\n",
            "Epoch: 0 [190/250] Loss: 2.4210\n",
            "Epoch: 0 [200/250] Loss: 2.3050\n",
            "Epoch: 0 [210/250] Loss: 2.3963\n",
            "Epoch: 0 [220/250] Loss: 2.3279\n",
            "Epoch: 0 [230/250] Loss: 2.3620\n",
            "Epoch: 0 [240/250] Loss: 2.3453\n",
            "Epoch: 0 [250/250] Loss: 2.3064\n",
            "Training completed - Accuracy: 9.93%, Avg Loss: 2.3619\n",
            "Training complete for epoch 5\n",
            "Epoch 5/10, Train Loss: 590.4786, Train Accuracy: 9.93%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.50%\n",
            "Validation complete for epoch 5\n",
            "Epoch 5/10, Validation Accuracy: 12.50%\n",
            "Epoch 5 completed in 154.40 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 6/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3578\n",
            "Epoch: 0 [20/250] Loss: 2.2520\n",
            "Epoch: 0 [30/250] Loss: 2.2923\n",
            "Epoch: 0 [40/250] Loss: 2.2806\n",
            "Epoch: 0 [50/250] Loss: 2.4098\n",
            "Epoch: 0 [60/250] Loss: 2.3164\n",
            "Epoch: 0 [70/250] Loss: 2.4964\n",
            "Epoch: 0 [80/250] Loss: 2.4103\n",
            "Epoch: 0 [90/250] Loss: 2.3341\n",
            "Epoch: 0 [100/250] Loss: 2.2807\n",
            "Epoch: 0 [110/250] Loss: 2.4572\n",
            "Epoch: 0 [120/250] Loss: 2.2952\n",
            "Epoch: 0 [130/250] Loss: 2.3239\n",
            "Epoch: 0 [140/250] Loss: 2.3763\n",
            "Epoch: 0 [150/250] Loss: 2.4390\n",
            "Epoch: 0 [160/250] Loss: 2.2944\n",
            "Epoch: 0 [170/250] Loss: 2.4170\n",
            "Epoch: 0 [180/250] Loss: 2.3706\n",
            "Epoch: 0 [190/250] Loss: 2.4182\n",
            "Epoch: 0 [200/250] Loss: 2.3086\n",
            "Epoch: 0 [210/250] Loss: 2.2571\n",
            "Epoch: 0 [220/250] Loss: 2.4162\n",
            "Epoch: 0 [230/250] Loss: 2.3992\n",
            "Epoch: 0 [240/250] Loss: 2.3486\n",
            "Epoch: 0 [250/250] Loss: 2.3114\n",
            "Training completed - Accuracy: 10.21%, Avg Loss: 2.3616\n",
            "Training complete for epoch 6\n",
            "Epoch 6/10, Train Loss: 590.4043, Train Accuracy: 10.21%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.20%\n",
            "Validation complete for epoch 6\n",
            "Epoch 6/10, Validation Accuracy: 12.20%\n",
            "Epoch 6 completed in 154.62 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 7/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.4068\n",
            "Epoch: 0 [20/250] Loss: 2.3566\n",
            "Epoch: 0 [30/250] Loss: 2.2277\n",
            "Epoch: 0 [40/250] Loss: 2.4583\n",
            "Epoch: 0 [50/250] Loss: 2.4055\n",
            "Epoch: 0 [60/250] Loss: 2.3578\n",
            "Epoch: 0 [70/250] Loss: 2.3032\n",
            "Epoch: 0 [80/250] Loss: 2.3417\n",
            "Epoch: 0 [90/250] Loss: 2.4103\n",
            "Epoch: 0 [100/250] Loss: 2.5277\n",
            "Epoch: 0 [110/250] Loss: 2.3317\n",
            "Epoch: 0 [120/250] Loss: 2.3829\n",
            "Epoch: 0 [130/250] Loss: 2.3052\n",
            "Epoch: 0 [140/250] Loss: 2.4178\n",
            "Epoch: 0 [150/250] Loss: 2.3728\n",
            "Epoch: 0 [160/250] Loss: 2.4028\n",
            "Epoch: 0 [170/250] Loss: 2.2911\n",
            "Epoch: 0 [180/250] Loss: 2.3452\n",
            "Epoch: 0 [190/250] Loss: 2.2914\n",
            "Epoch: 0 [200/250] Loss: 2.4040\n",
            "Epoch: 0 [210/250] Loss: 2.3229\n",
            "Epoch: 0 [220/250] Loss: 2.3481\n",
            "Epoch: 0 [230/250] Loss: 2.3900\n",
            "Epoch: 0 [240/250] Loss: 2.3174\n",
            "Epoch: 0 [250/250] Loss: 2.3349\n",
            "Training completed - Accuracy: 10.35%, Avg Loss: 2.3598\n",
            "Training complete for epoch 7\n",
            "Epoch 7/10, Train Loss: 589.9396, Train Accuracy: 10.35%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.40%\n",
            "Validation complete for epoch 7\n",
            "Epoch 7/10, Validation Accuracy: 12.40%\n",
            "Epoch 7 completed in 152.80 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 8/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3256\n",
            "Epoch: 0 [20/250] Loss: 2.3919\n",
            "Epoch: 0 [30/250] Loss: 2.4007\n",
            "Epoch: 0 [40/250] Loss: 2.3507\n",
            "Epoch: 0 [50/250] Loss: 2.2978\n",
            "Epoch: 0 [60/250] Loss: 2.3674\n",
            "Epoch: 0 [70/250] Loss: 2.3152\n",
            "Epoch: 0 [80/250] Loss: 2.3756\n",
            "Epoch: 0 [90/250] Loss: 2.4831\n",
            "Epoch: 0 [100/250] Loss: 2.3244\n",
            "Epoch: 0 [110/250] Loss: 2.3364\n",
            "Epoch: 0 [120/250] Loss: 2.3093\n",
            "Epoch: 0 [130/250] Loss: 2.2824\n",
            "Epoch: 0 [140/250] Loss: 2.4277\n",
            "Epoch: 0 [150/250] Loss: 2.3092\n",
            "Epoch: 0 [160/250] Loss: 2.4119\n",
            "Epoch: 0 [170/250] Loss: 2.4197\n",
            "Epoch: 0 [180/250] Loss: 2.4486\n",
            "Epoch: 0 [190/250] Loss: 2.3791\n",
            "Epoch: 0 [200/250] Loss: 2.2913\n",
            "Epoch: 0 [210/250] Loss: 2.3947\n",
            "Epoch: 0 [220/250] Loss: 2.4378\n",
            "Epoch: 0 [230/250] Loss: 2.4645\n",
            "Epoch: 0 [240/250] Loss: 2.3694\n",
            "Epoch: 0 [250/250] Loss: 2.2495\n",
            "Training completed - Accuracy: 9.85%, Avg Loss: 2.3662\n",
            "Training complete for epoch 8\n",
            "Epoch 8/10, Train Loss: 591.5592, Train Accuracy: 9.85%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.45%\n",
            "Validation complete for epoch 8\n",
            "Epoch 8/10, Validation Accuracy: 12.45%\n",
            "Epoch 8 completed in 152.78 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 9/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3488\n",
            "Epoch: 0 [20/250] Loss: 2.2833\n",
            "Epoch: 0 [30/250] Loss: 2.3278\n",
            "Epoch: 0 [40/250] Loss: 2.4532\n",
            "Epoch: 0 [50/250] Loss: 2.4131\n",
            "Epoch: 0 [60/250] Loss: 2.2368\n",
            "Epoch: 0 [70/250] Loss: 2.3599\n",
            "Epoch: 0 [80/250] Loss: 2.4103\n",
            "Epoch: 0 [90/250] Loss: 2.3762\n",
            "Epoch: 0 [100/250] Loss: 2.3113\n",
            "Epoch: 0 [110/250] Loss: 2.3935\n",
            "Epoch: 0 [120/250] Loss: 2.2701\n",
            "Epoch: 0 [130/250] Loss: 2.3327\n",
            "Epoch: 0 [140/250] Loss: 2.4860\n",
            "Epoch: 0 [150/250] Loss: 2.3668\n",
            "Epoch: 0 [160/250] Loss: 2.2881\n",
            "Epoch: 0 [170/250] Loss: 2.4168\n",
            "Epoch: 0 [180/250] Loss: 2.2411\n",
            "Epoch: 0 [190/250] Loss: 2.3952\n",
            "Epoch: 0 [200/250] Loss: 2.3370\n",
            "Epoch: 0 [210/250] Loss: 2.4170\n",
            "Epoch: 0 [220/250] Loss: 2.3448\n",
            "Epoch: 0 [230/250] Loss: 2.3935\n",
            "Epoch: 0 [240/250] Loss: 2.3149\n",
            "Epoch: 0 [250/250] Loss: 2.3651\n",
            "Training completed - Accuracy: 10.38%, Avg Loss: 2.3638\n",
            "Training complete for epoch 9\n",
            "Epoch 9/10, Train Loss: 590.9484, Train Accuracy: 10.38%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.25%\n",
            "Validation complete for epoch 9\n",
            "Epoch 9/10, Validation Accuracy: 12.25%\n",
            "Epoch 9 completed in 153.30 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 10/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3637\n",
            "Epoch: 0 [20/250] Loss: 2.3590\n",
            "Epoch: 0 [30/250] Loss: 2.2828\n",
            "Epoch: 0 [40/250] Loss: 2.2660\n",
            "Epoch: 0 [50/250] Loss: 2.3549\n",
            "Epoch: 0 [60/250] Loss: 2.3194\n",
            "Epoch: 0 [70/250] Loss: 2.4240\n",
            "Epoch: 0 [80/250] Loss: 2.2743\n",
            "Epoch: 0 [90/250] Loss: 2.1540\n",
            "Epoch: 0 [100/250] Loss: 2.4365\n",
            "Epoch: 0 [110/250] Loss: 2.3876\n",
            "Epoch: 0 [120/250] Loss: 2.2752\n",
            "Epoch: 0 [130/250] Loss: 2.2360\n",
            "Epoch: 0 [140/250] Loss: 2.3977\n",
            "Epoch: 0 [150/250] Loss: 2.3363\n",
            "Epoch: 0 [160/250] Loss: 2.3483\n",
            "Epoch: 0 [170/250] Loss: 2.3610\n",
            "Epoch: 0 [180/250] Loss: 2.2745\n",
            "Epoch: 0 [190/250] Loss: 2.3647\n",
            "Epoch: 0 [200/250] Loss: 2.4598\n",
            "Epoch: 0 [210/250] Loss: 2.5541\n",
            "Epoch: 0 [220/250] Loss: 2.2607\n",
            "Epoch: 0 [230/250] Loss: 2.4232\n",
            "Epoch: 0 [240/250] Loss: 2.3445\n",
            "Epoch: 0 [250/250] Loss: 2.4080\n",
            "Training completed - Accuracy: 9.81%, Avg Loss: 2.3646\n",
            "Training complete for epoch 10\n",
            "Epoch 10/10, Train Loss: 591.1481, Train Accuracy: 9.81%\n",
            "Starting validation...\n",
            "Validation Accuracy: 12.15%\n",
            "Validation complete for epoch 10\n",
            "Epoch 10/10, Validation Accuracy: 12.15%\n",
            "Epoch 10 completed in 152.72 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Model training and validation completed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_time</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▄▆▆▅▂▆█▁█▁</td></tr><tr><td>train_loss</td><td>▃▃▅█▃▂▁▆▄▄</td></tr><tr><td>val_accuracy</td><td>▃▁▆█▆▄▅▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>epoch_time</td><td>152.71961</td></tr><tr><td>train_accuracy</td><td>9.81373</td></tr><tr><td>train_loss</td><td>591.14812</td></tr><tr><td>val_accuracy</td><td>12.15</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ks[3, 5, 5, 7, 7]ac-silu_drop-0.3_fs-[32, 32, 32, 32, 32]_bn-Yes_dence-128</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/3z4tehjr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/3z4tehjr</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250416_083243-3z4tehjr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nx92y76v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: [128, 128, 64, 64, 32]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [5, 5, 5, 5, 5]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dense: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DL A2' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250416_094306-nx92y76v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/nx92y76v' target=\"_blank\">robust-sweep-2</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/65pmdmkr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/nx92y76v' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/nx92y76v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 250\n",
            "Validation data size: 63\n",
            "Starting model_train_val function\n",
            "Beginning training for 10 epochs\n",
            "Starting epoch 1/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3253\n",
            "Epoch: 0 [20/250] Loss: 2.3256\n",
            "Epoch: 0 [30/250] Loss: 2.4987\n",
            "Epoch: 0 [40/250] Loss: 2.3765\n",
            "Epoch: 0 [50/250] Loss: 2.4426\n",
            "Epoch: 0 [60/250] Loss: 2.3667\n",
            "Epoch: 0 [70/250] Loss: 2.4269\n",
            "Epoch: 0 [80/250] Loss: 2.3560\n",
            "Epoch: 0 [90/250] Loss: 2.4383\n",
            "Epoch: 0 [100/250] Loss: 2.3689\n",
            "Epoch: 0 [110/250] Loss: 2.3276\n",
            "Epoch: 0 [120/250] Loss: 2.3698\n",
            "Epoch: 0 [130/250] Loss: 2.3546\n",
            "Epoch: 0 [140/250] Loss: 2.4239\n",
            "Epoch: 0 [150/250] Loss: 2.2556\n",
            "Epoch: 0 [160/250] Loss: 2.3860\n",
            "Epoch: 0 [170/250] Loss: 2.4016\n",
            "Epoch: 0 [180/250] Loss: 2.3100\n",
            "Epoch: 0 [190/250] Loss: 2.5424\n",
            "Epoch: 0 [200/250] Loss: 2.3582\n",
            "Epoch: 0 [210/250] Loss: 2.4372\n",
            "Epoch: 0 [220/250] Loss: 2.3425\n",
            "Epoch: 0 [230/250] Loss: 2.3198\n",
            "Epoch: 0 [240/250] Loss: 2.5009\n",
            "Epoch: 0 [250/250] Loss: 2.4475\n",
            "Training completed - Accuracy: 9.66%, Avg Loss: 2.3748\n",
            "Training complete for epoch 1\n",
            "Epoch 1/10, Train Loss: 593.6988, Train Accuracy: 9.66%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.75%\n",
            "Validation complete for epoch 1\n",
            "Epoch 1/10, Validation Accuracy: 10.75%\n",
            "Epoch 1 completed in 197.77 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 2/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3910\n",
            "Epoch: 0 [20/250] Loss: 2.4289\n",
            "Epoch: 0 [30/250] Loss: 2.4625\n",
            "Epoch: 0 [40/250] Loss: 2.3635\n",
            "Epoch: 0 [50/250] Loss: 2.3654\n",
            "Epoch: 0 [60/250] Loss: 2.2283\n",
            "Epoch: 0 [70/250] Loss: 2.3468\n",
            "Epoch: 0 [80/250] Loss: 2.3971\n",
            "Epoch: 0 [90/250] Loss: 2.4360\n",
            "Epoch: 0 [100/250] Loss: 2.3706\n",
            "Epoch: 0 [110/250] Loss: 2.4401\n",
            "Epoch: 0 [120/250] Loss: 2.4176\n",
            "Epoch: 0 [130/250] Loss: 2.3837\n",
            "Epoch: 0 [140/250] Loss: 2.3760\n",
            "Epoch: 0 [150/250] Loss: 2.3954\n",
            "Epoch: 0 [160/250] Loss: 2.3436\n",
            "Epoch: 0 [170/250] Loss: 2.4009\n",
            "Epoch: 0 [180/250] Loss: 2.3313\n",
            "Epoch: 0 [190/250] Loss: 2.4340\n",
            "Epoch: 0 [200/250] Loss: 2.2802\n",
            "Epoch: 0 [210/250] Loss: 2.5623\n",
            "Epoch: 0 [220/250] Loss: 2.5275\n",
            "Epoch: 0 [230/250] Loss: 2.2968\n",
            "Epoch: 0 [240/250] Loss: 2.3621\n",
            "Epoch: 0 [250/250] Loss: 2.3565\n",
            "Training completed - Accuracy: 9.41%, Avg Loss: 2.3807\n",
            "Training complete for epoch 2\n",
            "Epoch 2/10, Train Loss: 595.1814, Train Accuracy: 9.41%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.15%\n",
            "Validation complete for epoch 2\n",
            "Epoch 2/10, Validation Accuracy: 10.15%\n",
            "Epoch 2 completed in 192.13 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 3/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3074\n",
            "Epoch: 0 [20/250] Loss: 2.4047\n",
            "Epoch: 0 [30/250] Loss: 2.3645\n",
            "Epoch: 0 [40/250] Loss: 2.4748\n",
            "Epoch: 0 [50/250] Loss: 2.3146\n",
            "Epoch: 0 [60/250] Loss: 2.4071\n",
            "Epoch: 0 [70/250] Loss: 2.5014\n",
            "Epoch: 0 [80/250] Loss: 2.4830\n",
            "Epoch: 0 [90/250] Loss: 2.4235\n",
            "Epoch: 0 [100/250] Loss: 2.3730\n",
            "Epoch: 0 [110/250] Loss: 2.4998\n",
            "Epoch: 0 [120/250] Loss: 2.3205\n",
            "Epoch: 0 [130/250] Loss: 2.4769\n",
            "Epoch: 0 [140/250] Loss: 2.2917\n",
            "Epoch: 0 [150/250] Loss: 2.3232\n",
            "Epoch: 0 [160/250] Loss: 2.3051\n",
            "Epoch: 0 [170/250] Loss: 2.6123\n",
            "Epoch: 0 [180/250] Loss: 2.3075\n",
            "Epoch: 0 [190/250] Loss: 2.4910\n",
            "Epoch: 0 [200/250] Loss: 2.3895\n",
            "Epoch: 0 [210/250] Loss: 2.3026\n",
            "Epoch: 0 [220/250] Loss: 2.3630\n",
            "Epoch: 0 [230/250] Loss: 2.3619\n",
            "Epoch: 0 [240/250] Loss: 2.4059\n",
            "Epoch: 0 [250/250] Loss: 2.2325\n",
            "Training completed - Accuracy: 9.80%, Avg Loss: 2.3759\n",
            "Training complete for epoch 3\n",
            "Epoch 3/10, Train Loss: 593.9794, Train Accuracy: 9.80%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.75%\n",
            "Validation complete for epoch 3\n",
            "Epoch 3/10, Validation Accuracy: 10.75%\n",
            "Epoch 3 completed in 192.15 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 4/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.4681\n",
            "Epoch: 0 [20/250] Loss: 2.5000\n",
            "Epoch: 0 [30/250] Loss: 2.2731\n",
            "Epoch: 0 [40/250] Loss: 2.4221\n",
            "Epoch: 0 [50/250] Loss: 2.2910\n",
            "Epoch: 0 [60/250] Loss: 2.2972\n",
            "Epoch: 0 [70/250] Loss: 2.2877\n",
            "Epoch: 0 [80/250] Loss: 2.3795\n",
            "Epoch: 0 [90/250] Loss: 2.4685\n",
            "Epoch: 0 [100/250] Loss: 2.4276\n",
            "Epoch: 0 [110/250] Loss: 2.2969\n",
            "Epoch: 0 [120/250] Loss: 2.4687\n",
            "Epoch: 0 [130/250] Loss: 2.3393\n",
            "Epoch: 0 [140/250] Loss: 2.2542\n",
            "Epoch: 0 [150/250] Loss: 2.3508\n",
            "Epoch: 0 [160/250] Loss: 2.3387\n",
            "Epoch: 0 [170/250] Loss: 2.2925\n",
            "Epoch: 0 [180/250] Loss: 2.3417\n",
            "Epoch: 0 [190/250] Loss: 2.4029\n",
            "Epoch: 0 [200/250] Loss: 2.2897\n",
            "Epoch: 0 [210/250] Loss: 2.5091\n",
            "Epoch: 0 [220/250] Loss: 2.2532\n",
            "Epoch: 0 [230/250] Loss: 2.3248\n",
            "Epoch: 0 [240/250] Loss: 2.2744\n",
            "Epoch: 0 [250/250] Loss: 2.3631\n",
            "Training completed - Accuracy: 10.26%, Avg Loss: 2.3724\n",
            "Training complete for epoch 4\n",
            "Epoch 4/10, Train Loss: 593.1087, Train Accuracy: 10.26%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.65%\n",
            "Validation complete for epoch 4\n",
            "Epoch 4/10, Validation Accuracy: 10.65%\n",
            "Epoch 4 completed in 192.55 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 5/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3739\n",
            "Epoch: 0 [20/250] Loss: 2.4538\n",
            "Epoch: 0 [30/250] Loss: 2.4394\n",
            "Epoch: 0 [40/250] Loss: 2.3126\n",
            "Epoch: 0 [50/250] Loss: 2.3950\n",
            "Epoch: 0 [60/250] Loss: 2.3705\n",
            "Epoch: 0 [70/250] Loss: 2.3774\n",
            "Epoch: 0 [80/250] Loss: 2.3761\n",
            "Epoch: 0 [90/250] Loss: 2.3798\n",
            "Epoch: 0 [100/250] Loss: 2.4619\n",
            "Epoch: 0 [110/250] Loss: 2.2448\n",
            "Epoch: 0 [120/250] Loss: 2.5072\n",
            "Epoch: 0 [130/250] Loss: 2.4211\n",
            "Epoch: 0 [140/250] Loss: 2.4430\n",
            "Epoch: 0 [150/250] Loss: 2.3396\n",
            "Epoch: 0 [160/250] Loss: 2.3476\n",
            "Epoch: 0 [170/250] Loss: 2.3946\n",
            "Epoch: 0 [180/250] Loss: 2.3680\n",
            "Epoch: 0 [190/250] Loss: 2.3019\n",
            "Epoch: 0 [200/250] Loss: 2.4062\n",
            "Epoch: 0 [210/250] Loss: 2.3063\n",
            "Epoch: 0 [220/250] Loss: 2.4764\n",
            "Epoch: 0 [230/250] Loss: 2.2974\n",
            "Epoch: 0 [240/250] Loss: 2.4195\n",
            "Epoch: 0 [250/250] Loss: 2.4361\n",
            "Training completed - Accuracy: 9.76%, Avg Loss: 2.3745\n",
            "Training complete for epoch 5\n",
            "Epoch 5/10, Train Loss: 593.6203, Train Accuracy: 9.76%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.40%\n",
            "Validation complete for epoch 5\n",
            "Epoch 5/10, Validation Accuracy: 10.40%\n",
            "Epoch 5 completed in 193.99 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 6/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3415\n",
            "Epoch: 0 [20/250] Loss: 2.3190\n",
            "Epoch: 0 [30/250] Loss: 2.3785\n",
            "Epoch: 0 [40/250] Loss: 2.3389\n",
            "Epoch: 0 [50/250] Loss: 2.4301\n",
            "Epoch: 0 [60/250] Loss: 2.3020\n",
            "Epoch: 0 [70/250] Loss: 2.3547\n",
            "Epoch: 0 [80/250] Loss: 2.4145\n",
            "Epoch: 0 [90/250] Loss: 2.3658\n",
            "Epoch: 0 [100/250] Loss: 2.3350\n",
            "Epoch: 0 [110/250] Loss: 2.4063\n",
            "Epoch: 0 [120/250] Loss: 2.3544\n",
            "Epoch: 0 [130/250] Loss: 2.2358\n",
            "Epoch: 0 [140/250] Loss: 2.3232\n",
            "Epoch: 0 [150/250] Loss: 2.3210\n",
            "Epoch: 0 [160/250] Loss: 2.4275\n",
            "Epoch: 0 [170/250] Loss: 2.3993\n",
            "Epoch: 0 [180/250] Loss: 2.3415\n",
            "Epoch: 0 [190/250] Loss: 2.3526\n",
            "Epoch: 0 [200/250] Loss: 2.4275\n",
            "Epoch: 0 [210/250] Loss: 2.3008\n",
            "Epoch: 0 [220/250] Loss: 2.3128\n",
            "Epoch: 0 [230/250] Loss: 2.3965\n",
            "Epoch: 0 [240/250] Loss: 2.4921\n",
            "Epoch: 0 [250/250] Loss: 2.3197\n",
            "Training completed - Accuracy: 9.53%, Avg Loss: 2.3768\n",
            "Training complete for epoch 6\n",
            "Epoch 6/10, Train Loss: 594.2036, Train Accuracy: 9.53%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.45%\n",
            "Validation complete for epoch 6\n",
            "Epoch 6/10, Validation Accuracy: 10.45%\n",
            "Epoch 6 completed in 193.02 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 7/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3893\n",
            "Epoch: 0 [20/250] Loss: 2.3984\n",
            "Epoch: 0 [30/250] Loss: 2.3623\n",
            "Epoch: 0 [40/250] Loss: 2.2513\n",
            "Epoch: 0 [50/250] Loss: 2.3444\n",
            "Epoch: 0 [60/250] Loss: 2.3609\n",
            "Epoch: 0 [70/250] Loss: 2.3763\n",
            "Epoch: 0 [80/250] Loss: 2.2780\n",
            "Epoch: 0 [90/250] Loss: 2.4046\n",
            "Epoch: 0 [100/250] Loss: 2.2960\n",
            "Epoch: 0 [110/250] Loss: 2.3338\n",
            "Epoch: 0 [120/250] Loss: 2.3873\n",
            "Epoch: 0 [130/250] Loss: 2.3766\n",
            "Epoch: 0 [140/250] Loss: 2.3708\n",
            "Epoch: 0 [150/250] Loss: 2.4137\n",
            "Epoch: 0 [160/250] Loss: 2.4278\n",
            "Epoch: 0 [170/250] Loss: 2.4114\n",
            "Epoch: 0 [180/250] Loss: 2.3074\n",
            "Epoch: 0 [190/250] Loss: 2.4472\n",
            "Epoch: 0 [200/250] Loss: 2.2794\n",
            "Epoch: 0 [210/250] Loss: 2.5299\n",
            "Epoch: 0 [220/250] Loss: 2.3792\n",
            "Epoch: 0 [230/250] Loss: 2.3480\n",
            "Epoch: 0 [240/250] Loss: 2.3273\n",
            "Epoch: 0 [250/250] Loss: 2.3779\n",
            "Training completed - Accuracy: 9.34%, Avg Loss: 2.3842\n",
            "Training complete for epoch 7\n",
            "Epoch 7/10, Train Loss: 596.0399, Train Accuracy: 9.34%\n",
            "Starting validation...\n",
            "Validation Accuracy: 10.55%\n",
            "Validation complete for epoch 7\n",
            "Epoch 7/10, Validation Accuracy: 10.55%\n",
            "Epoch 7 completed in 192.24 seconds\n",
            "Logging to wandb...\n",
            "Successfully logged to wandb\n",
            "Starting epoch 8/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.3417\n",
            "Epoch: 0 [20/250] Loss: 2.4548\n",
            "Epoch: 0 [30/250] Loss: 2.3258\n",
            "Epoch: 0 [40/250] Loss: 2.3581\n",
            "Epoch: 0 [50/250] Loss: 2.3103\n",
            "Epoch: 0 [60/250] Loss: 2.4154\n",
            "Epoch: 0 [70/250] Loss: 2.2683\n",
            "Epoch: 0 [80/250] Loss: 2.4752\n",
            "Epoch: 0 [90/250] Loss: 2.4316\n",
            "Epoch: 0 [100/250] Loss: 2.3631\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    with wandb.init(project='DL A2') as run:\n",
        "        run_name=\"ks\"+str(wandb.config.kernel_size)+\"ac-\"+(wandb.config.activation)+\"_drop-\"+str(wandb.config.dropout)+\"_fs-\"+str(wandb.config.filter_org)+\"_bn-\"+str(wandb.config.batch_norm)+\"_dence-\"+str(wandb.config.num_dense)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "\n",
        "        if  wandb.config.activation == 'relu':\n",
        "            activ=nn.ReLU()\n",
        "        elif wandb.config.activation == 'gelu':\n",
        "            activ=nn.GELU()\n",
        "        elif wandb.config.activation == 'silu':\n",
        "            activ=nn.SiLU()\n",
        "        elif wandb.config.activation == 'mish':\n",
        "            activ=nn.Mish()\n",
        "\n",
        "        model_= model(num_filters=wandb.config.filter_org, filter_size=wandb.config.kernel_size,\n",
        "                      activation=activ, stride=1,padding=1, pool_size=(2,2), fc_size=wandb.config.num_dense,\n",
        "                      nom_o_classes=10,dropout = wandb.config.dropout).to(device)\n",
        "\n",
        "        train, validation = train_data(train_data_dir,data_augmentation= wandb.config.data_aug)\n",
        "\n",
        "        model_train_val(model=model_, train_data=train, val_data=validation, epochs = 10)\n",
        "\n",
        "wandb.agent(sweep_id, function= main,count= 5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GhO8Nx22gW1t-cL9NZnp_rNiIURNdbqj",
      "authorship_tag": "ABX9TyOOYr6ObDJl3mG+x7rTX8e+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}