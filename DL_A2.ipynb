{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazal735/DL_A2/blob/main/DL_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gH4UR8swGpVy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyAQJsl3nQpL",
        "outputId": "34699752-2399-4e78-8f6b-ffe40fb2905b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "38ChQLPx-Kta"
      },
      "outputs": [],
      "source": [
        "train_data_dir = '/content/drive/MyDrive/inaturalist_12K/train'\n",
        "test_data_dir = '/content/drive/MyDrive/inaturalist_12K/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F9B_ez4BtbKJ"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-7pou-cN_xvq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train data loading\n",
        "def train_data(train_data_dir,data_augmentation):\n",
        "  size=transforms.Resize((224,224))\n",
        "  to_tensor=transforms.ToTensor()\n",
        "  #check again-autogenerated\n",
        "  normalize=transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "  crop=transforms.RandomResizedCrop(224)\n",
        "  flip=transforms.RandomHorizontalFlip()\n",
        "  #try changing the values\n",
        "  color=transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "  rotation=transforms.RandomRotation(20)\n",
        "\n",
        "\n",
        "  #augmentation of image\n",
        "  if data_augmentation == 'Yes':\n",
        "        transform_img = transforms.Compose([crop,flip,color, rotation, to_tensor,normalize]) # Data transformations\n",
        "\n",
        "  else:\n",
        "      transform_img = transforms.Compose([size,to_tensor, normalize ])\n",
        "\n",
        "\n",
        "\n",
        "  #data fetchiing\n",
        "  training_data=torchvision.datasets.ImageFolder(train_data_dir, transform=transform_img)\n",
        "\n",
        "\n",
        "  train_index, val_index = train_test_split(list(range(len(training_data))), test_size=0.2, random_state=42)\n",
        "  train_sampler = SubsetRandomSampler(train_index)\n",
        "  val_sampler = SubsetRandomSampler(val_index)\n",
        "\n",
        "  train_data=DataLoader(training_data,batch_size=32,sampler=train_sampler)\n",
        "  validation_data = DataLoader(training_data, batch_size=32, sampler=val_sampler)\n",
        "  print('Train data size:', len(train_data))\n",
        "  print('Validation data size:', len(validation_data))\n",
        "\n",
        "  return train_data,validation_data\n",
        "\n",
        "#test data loading\n",
        "def test_data(test_data_dir,data_augmentation):\n",
        "  size=transforms.Resize((224,224))\n",
        "  to_tensor=transforms.ToTensor()\n",
        "  #check again-autogenerated\n",
        "  normalize=transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "\n",
        "\n",
        "  #augmentation of image\n",
        "  data_transform=transforms.Compose([size,to_tensor,normalize])\n",
        "\n",
        "  #data fetching\n",
        "  test_data=torchvision.datasets.ImageFolder(test_data_dir,transform=data_transform)\n",
        "  test_data=DataLoader(test_data,batch_size=32)\n",
        "\n",
        "\n",
        "  return test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJKcqO-0Ku_"
      },
      "source": [
        "model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn01KxwgGnYh",
        "outputId": "592e1d41-6299-4515-efbb-4c26c7f7f7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model(\n",
            "  (activation): ReLU()\n",
            "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout2): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout3): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout4): Dropout2d(p=0, inplace=False)\n",
            "  (conv_layer5): Conv2d(256, 512, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout5): Dropout2d(p=0, inplace=False)\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout_layer): Dropout1d(p=0, inplace=False)\n",
            "  (fc): Linear(in_features=8192, out_features=512, bias=True)\n",
            "  (fc_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (output_layer): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self,num_filters=[32,64,128,256,512],filter_size=[3,3,5,5,7],activation=nn.ReLU(),\n",
        "               stride=1, padding=1, pool_size=(2,2),fc_size=512,nom_o_classes=10,\n",
        "               dropout=0,in_channels=3,batch_norm='YES'):\n",
        "    super(model,self).__init__()\n",
        "    self.num_filters=num_filters\n",
        "    self.filter_size=filter_size\n",
        "    self.activation=activation\n",
        "    self.stride=stride\n",
        "    self.padding=padding\n",
        "    self.pool_size=pool_size\n",
        "    self.fc_size=fc_size\n",
        "    self.nom_o_classes=nom_o_classes\n",
        "    self.dropout=dropout\n",
        "    self.channels=in_channels\n",
        "\n",
        "\n",
        "    def image_size(img_w,filter_size,padding,stride):\n",
        "      return ((img_w-filter_size+2*padding)/stride+1)*0.5\n",
        "\n",
        "\n",
        "    #layers of convolution\n",
        "    #layer1\n",
        "    self.conv_layer1=nn.Conv2d(self.channels,self.num_filters[0], stride=self.stride, padding=self.padding,\n",
        "                               kernel_size=self.filter_size[0])\n",
        "    self.batch_norm1=nn.BatchNorm2d(self.num_filters[0])\n",
        "    #self.dropout1=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size1=image_size(224,self.filter_size[0],self.padding,self.stride)\n",
        "\n",
        "    #layer2\n",
        "    self.conv_layer2=nn.Conv2d(self.num_filters[0],self.num_filters[1], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[1])\n",
        "    self.batch_norm2=nn.BatchNorm2d(self.num_filters[1])\n",
        "    self.dropout2=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size2=image_size(img_size1,self.filter_size[1],self.padding,self.stride)\n",
        "\n",
        "    #layer3\n",
        "    self.conv_layer3=nn.Conv2d(self.num_filters[1],self.num_filters[2], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[2])\n",
        "    self.batch_norm3=nn.BatchNorm2d(self.num_filters[2])\n",
        "    self.dropout3=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size3=image_size(img_size2,self.filter_size[2],self.padding,self.stride)\n",
        "\n",
        "    #layer4\n",
        "    self.conv_layer4=nn.Conv2d(self.num_filters[2],self.num_filters[3], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[3])\n",
        "    self.batch_norm4=nn.BatchNorm2d(self.num_filters[3])\n",
        "    self.dropout4=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size4=image_size(img_size3,self.filter_size[3],self.padding,self.stride)\n",
        "\n",
        "    #layer5\n",
        "    self.conv_layer5=nn.Conv2d(self.num_filters[3],self.num_filters[4], stride=self.stride, padding=self.padding,\n",
        "                              kernel_size=self.filter_size[4])\n",
        "    self.batch_norm5=nn.BatchNorm2d(self.num_filters[4])\n",
        "    self.dropout5=nn.Dropout2d(self.dropout)\n",
        "\n",
        "    img_size5=int(image_size(img_size4,self.filter_size[4],self.padding,self.stride))\n",
        "\n",
        "\n",
        "    self.pool=nn.MaxPool2d(self.pool_size,stride=2)\n",
        "\n",
        "    self.dropout_layer = nn.Dropout1d(self.dropout)\n",
        "\n",
        "    # Define fully connected layer\n",
        "    self.fc = nn.Linear(self.num_filters[4] * (img_size5 ** 2), self.fc_size)\n",
        "    self.fc_bn = nn.BatchNorm1d(self.fc_size)  # Batch normalization for fully connected layer\n",
        "\n",
        "    # Output layer\n",
        "    self.output_layer = nn.Linear(self.fc_size, self.nom_o_classes)\n",
        "\n",
        "    # forward\n",
        "  def forward(self,x):\n",
        "    #layer1\n",
        "    x=self.conv_layer1(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    #x=self.dropout1(x)\n",
        "    x=self.batch_norm1(x)\n",
        "\n",
        "      #layer2\n",
        "    x=self.conv_layer2(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.batch_norm2(x)\n",
        "\n",
        "    #layer3\n",
        "    x=self.conv_layer3(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout3(x)\n",
        "    x=self.batch_norm3(x)\n",
        "\n",
        "    #layer4\n",
        "    x=self.conv_layer4(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout4(x)\n",
        "    x=self.batch_norm4(x)\n",
        "\n",
        "    #layer5\n",
        "    x=self.conv_layer5(x)\n",
        "    x=self.activation(x)\n",
        "    x=self.pool(x)\n",
        "    x=self.dropout5(x)\n",
        "    x=self.batch_norm5(x)\n",
        "\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "    x = self.fc_bn(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.dropout_layer(x)\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model1=model()\n",
        "model1.to(device)\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZOrkblSLG7sz"
      },
      "outputs": [],
      "source": [
        "epochs=100\n",
        "learning_rate=0.1\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAJDHetA0Akv"
      },
      "source": [
        "model training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JSNnQI40HJI0"
      },
      "outputs": [],
      "source": [
        "# loss_metric=nn.CrossEntropyLoss()\n",
        "# optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)\n",
        "\n",
        "# def training(model1,data):\n",
        "\n",
        "#   model1.train(True)\n",
        "#   training_loss=0.0\n",
        "#   true_label=0\n",
        "#   total_train=0\n",
        "\n",
        "#   for input, label in data:\n",
        "#     input = input.to(device)\n",
        "#     label = label.to(device)\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     output=model1(input)\n",
        "#     loss=loss_metric(output,label)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     training_loss += loss.item()\n",
        "#     _,predicted=torch.max(output.data,1)\n",
        "#     total_train += label.size(0)\n",
        "#     true_label += (predicted==label).sum().item()\n",
        "\n",
        "#   train_accuracy=100*true_label/total_train\n",
        "#   return train_accuracy,training_loss,model1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2agnKN30E54"
      },
      "source": [
        "model testing function(on validation data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "97LWLz1CPXwY"
      },
      "outputs": [],
      "source": [
        "loss_metric=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model1.parameters(),lr=learning_rate)\n",
        "\n",
        "def training(model1, data, epoch=0):\n",
        "    model1.train(True)\n",
        "    training_loss = 0.0\n",
        "    true_label = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Add batch tracking\n",
        "    total_batches = len(data)\n",
        "\n",
        "    for batch_idx, (input, label) in enumerate(data):\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model1(input)\n",
        "        loss = loss_metric(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_train += label.size(0)\n",
        "        true_label += (predicted == label).sum().item()\n",
        "\n",
        "        # Print progress every few batches\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
        "            print(f'Epoch: {epoch} [{batch_idx+1}/{total_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "    train_accuracy = 100 * true_label / total_train\n",
        "    avg_loss = training_loss / total_batches\n",
        "\n",
        "    print(f\"Training completed - Accuracy: {train_accuracy:.2f}%, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return train_accuracy, training_loss, model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LtXvXgFRx4o5"
      },
      "outputs": [],
      "source": [
        "def test_on_valid_data(model, test_data):\n",
        "    model.eval()\n",
        "\n",
        "    correct_label = 0\n",
        "    total_label = 0\n",
        "    with torch.no_grad():\n",
        "        for img, label in test_data:\n",
        "            img, label = img.to(device), label.to(device)\n",
        "            output = model(img)\n",
        "\n",
        "            _, pred = torch.max(output, 1)\n",
        "            correct_label += (pred == label).sum().item()\n",
        "            total_label += label.size(0)\n",
        "\n",
        "    valid_accuracy = 100 * correct_label / total_label\n",
        "    print(f'Validation Accuracy: {valid_accuracy:.2f}%')\n",
        "    return valid_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LnUsHUP-6Aru"
      },
      "outputs": [],
      "source": [
        "def model_train_val(model, train_data, val_data,epochs,device=device):\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  print(\"model train val\")\n",
        "  for epoch in range(epochs):\n",
        "    train_accuracy,avg_loss,model1 = training(model, train_data)\n",
        "    # Print training loss and accuracy\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Validation loop\n",
        "    val_accuracy = test_on_valid_data(model, val_data)\n",
        "    # Print validation accuracy\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    wandb.log({'Train loss': avg_loss})\n",
        "    wandb.log({'Train accuracy': train_accuracy})\n",
        "\n",
        "    wandb.log({'val_accuracy': val_accuracy})\n",
        "    wandb.log({'epoch': epoch})\n",
        "\n",
        "def model_train_test(model, train_data, test_data,epochs):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_accuracy,avg_loss,model1 = training(model, train_data)\n",
        "    # Print training loss and accuracy\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Validation loop\n",
        "    val_accuracy = test_on_valid_data(model, val_data)\n",
        "    # Print validation accuracy\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    wandb.log({'Train loss': avg_loss})\n",
        "    wandb.log({'Train accuracy': train_accuracy})\n",
        "\n",
        "    wandb.log({'val_accuracy': val_accuracy})\n",
        "    wandb.log({'epoch': epoch})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xzowtcjtKQZf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def model_train_val(model, train_data, val_data, epochs, device=device):\n",
        "    print(\"Starting model_train_val function\")\n",
        "\n",
        "    # Define criterion and optimizer here\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Pass these to your training function if needed\n",
        "    # Or ensure global variables are properly synchronized\n",
        "\n",
        "    print(f\"Beginning training for {epochs} epochs\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Capture start time to monitor duration\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            print(\"Calling training function...\")\n",
        "            train_accuracy, avg_loss, model = training(model, train_data)\n",
        "            print(f\"Training complete for epoch {epoch+1}\")\n",
        "\n",
        "            # Print training metrics\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "            print(\"Starting validation...\")\n",
        "            val_accuracy = test_on_valid_data(model, val_data)\n",
        "            print(f\"Validation complete for epoch {epoch+1}\")\n",
        "\n",
        "            # Print validation metrics\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "            # Calculate epoch duration\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
        "\n",
        "            # Log to wandb with try-except to catch any logging errors\n",
        "            try:\n",
        "                print(\"Logging to wandb...\")\n",
        "                wandb.log({\n",
        "                    'epoch': epoch,\n",
        "                    'train_loss': avg_loss,\n",
        "                    'train_accuracy': train_accuracy,\n",
        "                    'val_accuracy': val_accuracy,\n",
        "                    'epoch_time': epoch_time\n",
        "                })\n",
        "                print(\"Successfully logged to wandb\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error logging to wandb: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during epoch {epoch+1}: {str(e)}\")\n",
        "            traceback.print_exc()  # This will print the full stack trace\n",
        "\n",
        "    print(\"Model training and validation completed\")\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7rR72xABts",
        "outputId": "3583d119-8eb3-4afe-fa2c-27d3879d4f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beUglxun8oZR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKO6dIE-q2Oy"
      },
      "source": [
        "e4d0a8c3ccaf2534e9ab91c659e420ba5114533f\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "OlwujivySuZk",
        "outputId": "5b8aa0f0-7cda-4bd6-9912-64b82cc2233c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmfazal735\u001b[0m (\u001b[33mmfazal735-iit-madras-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwKIfz6h8Gx1",
        "outputId": "d31cf744-cfdc-4c14-89d5-7f128cdc3a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 2hsjgo25\n",
            "Sweep URL: https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/2hsjgo25\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'kernel_size':{\n",
        "            'values': [[3,3,3,3,3],[3,5,5,7,7],[3,5,3,5,7],[5,5,5,5,5]]#,[7,7,7,7,7]]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.3, 0.2]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': [ 'relu','mish','silu', 'gelu',]\n",
        "        },\n",
        "        'num_dense':{\n",
        "            'values': [128, 256]\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': ['Yes','No']\n",
        "        },\n",
        "        'filter_org':{\n",
        "            'values': [[128,128,64,64,32],[32,64,128,256,512],[32,32,32,32,32],[32,64,64,128,128]]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values': [0.001,0.0001]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': ['Adam','SGD']\n",
        "        },\n",
        "        'data_aug': {\n",
        "            'values': ['No', 'Yes']\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL A2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "5AoQOnSN9LyE",
        "outputId": "452dbb5f-2b52-4d5f-c673-10ee0824f708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tez1xjb0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: Yes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: [32, 64, 64, 128, 128]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 5, 3, 5, 7]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dense: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DL A2' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250418_152407-tez1xjb0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/tez1xjb0' target=\"_blank\">volcanic-sweep-1</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/2hsjgo25' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/2hsjgo25</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/2hsjgo25' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/sweeps/2hsjgo25</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/tez1xjb0' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/DL%20A2/runs/tez1xjb0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 250\n",
            "Validation data size: 63\n",
            "Starting model_train_val function\n",
            "Beginning training for 10 epochs\n",
            "Starting epoch 1/10\n",
            "Calling training function...\n",
            "Epoch: 0 [10/250] Loss: 2.4601\n",
            "Epoch: 0 [20/250] Loss: 2.4936\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    with wandb.init(project='DL A2') as run:\n",
        "        run_name=\"ks\"+str(wandb.config.kernel_size)+\"ac-\"+(wandb.config.activation)+\"_drop-\"+str(wandb.config.dropout)+\"_fs-\"+str(wandb.config.filter_org)+\"_bn-\"+str(wandb.config.batch_norm)+\"_dence-\"+str(wandb.config.num_dense)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "\n",
        "        if  wandb.config.activation == 'relu':\n",
        "            activ=nn.ReLU()\n",
        "        elif wandb.config.activation == 'gelu':\n",
        "            activ=nn.GELU()\n",
        "        elif wandb.config.activation == 'silu':\n",
        "            activ=nn.SiLU()\n",
        "        elif wandb.config.activation == 'mish':\n",
        "            activ=nn.Mish()\n",
        "\n",
        "        model_= model(num_filters=wandb.config.filter_org, filter_size=wandb.config.kernel_size,\n",
        "                      activation=activ, stride=1,padding=1, pool_size=(2,2), fc_size=wandb.config.num_dense,\n",
        "                      nom_o_classes=10,dropout = wandb.config.dropout).to(device)\n",
        "\n",
        "        train, validation = train_data(train_data_dir,data_augmentation= wandb.config.data_aug)\n",
        "\n",
        "        model_train_val(model=model_, train_data=train, val_data=validation, epochs = 10)\n",
        "\n",
        "wandb.agent(sweep_id, function= main,count= 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XdRzFkn-eM-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'kernel_size':{\n",
        "            'values': [[3,3,3,3,3],[3,5,5,7,7],[3,5,3,5,7],[5,5,5,5,5]]#,[7,7,7,7,7]]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.3, 0.2]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': [ 'relu','mish','silu', 'gelu',]\n",
        "        },\n",
        "        'num_dense':{\n",
        "            'values': [128, 256]\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': ['Yes','No']\n",
        "        },\n",
        "        'filter_org':{\n",
        "            'values': [[128,128,64,64,32],[32,64,128,256,512],[32,32,32,32,32],[32,64,64,128,128]]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values': [0.001,0.0001]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': ['Adam','SGD']\n",
        "        },\n",
        "        'data_aug': {\n",
        "            'values': ['No', 'Yes']\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL A2')"
      ],
      "metadata": {
        "id": "G9WsvuaDgGRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    with wandb.init(project='DL A2') as run:\n",
        "        run_name=\"ks\"+str(wandb.config.kernel_size)+\"ac-\"+(wandb.config.activation)+\"_drop-\"+str(wandb.config.dropout)+\"_fs-\"+str(wandb.config.filter_org)+\"_bn-\"+str(wandb.config.batch_norm)+\"_dence-\"+str(wandb.config.num_dense)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "\n",
        "        if  wandb.config.activation == 'relu':\n",
        "            activ=nn.ReLU()\n",
        "        elif wandb.config.activation == 'gelu':\n",
        "            activ=nn.GELU()\n",
        "        elif wandb.config.activation == 'silu':\n",
        "            activ=nn.SiLU()\n",
        "        elif wandb.config.activation == 'mish':\n",
        "            activ=nn.Mish()\n",
        "\n",
        "        model_= model(num_filters=wandb.config.filter_org, filter_size=wandb.config.kernel_size,\n",
        "                      activation=activ, stride=1,padding=1, pool_size=(2,2), fc_size=wandb.config.num_dense,\n",
        "                      nom_o_classes=10,dropout = wandb.config.dropout).to(device)\n",
        "\n",
        "        train, validation = train_data(train_data_dir,data_augmentation= wandb.config.data_aug)\n",
        "\n",
        "        model_train_val(model=model_, train_data=train, val_data=validation, epochs = 10)\n",
        "\n",
        "        model_train_test(model=model_, train_data=test, val_data=validation, epochs = 10)\n",
        "\n",
        "wandb.agent(sweep_id, function= main,count= 5)"
      ],
      "metadata": {
        "id": "7yXcu70i6ATl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SRfD2NQE8nGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "a7c7c54e-f875-40d2-d8c0-20d62e5d3372"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-81470ccadd2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mimg_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m# wandb.log({\"images\": [wandb.Image(image, caption=\"Test image prediction\")]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_' is not defined"
          ]
        }
      ],
      "source": [
        "import imageio.v2\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def img_plot(model):\n",
        "    #For plotting random Images\n",
        "    classes_list = ['Amphibia','Animalia','Arachnida','Aves','Fungi','Insecta','Mammalia','Mollusca','Plantae','Reptilia']\n",
        "    # Directory of the test data\n",
        "    dir = '/content/drive/MyDrive/inaturalist_12K/val'\n",
        "    images = []  # List to store the images\n",
        "    labels = []  #  List to store the labels\n",
        "\n",
        "    for label, name in enumerate(classes_list):\n",
        "      images_list = os.listdir(dir+'/'+name)\n",
        "      img_names = []\n",
        "      for j in range(3):\n",
        "        index = random.randint(0, 199)\n",
        "        img_names.append(images_list[index])\n",
        "      for image_name in img_names:\n",
        "          image = imageio.v2.imread(dir+'/'+name+'/'+image_name)\n",
        "          if np.ndim(image) == 3:\n",
        "            images.append(cv2.resize(image, (224,224)))\n",
        "            labels.append(classes_list[label])\n",
        "\n",
        "    arr = np.array(images)\n",
        "    arr = (arr/255.0).astype('float32')\n",
        "    transp_arr = np.transpose(arr, (0, 3, 1, 2))\n",
        "    tensor1 = torch.tensor(transp_arr).to(device)\n",
        "    y_pred = model(tensor1)\n",
        "    y_pred_label = torch.argmax(y_pred, 1)\n",
        "    dictn = {0:'Amphibia',1:'Animalia',2:'Arachnida',3:'Aves',4:'Fungi',\n",
        "               5:'Insecta',6:'Mammalia',7:'Mollusca',8:'Plantae',9:'Reptilia'}\n",
        "\n",
        "    fig, axes = plt.subplots(10, 3, figsize=(12, 20))\n",
        "    for i in range(10):\n",
        "        for j in range(3):\n",
        "            ind = i * 3 + j\n",
        "            if ind < 30:\n",
        "                axes[i, j].imshow(images[ind])\n",
        "                axes[i, j].set_title(f'True: {labels[ind]}, Predicted: {dictn[y_pred_label[idx].item()]}')\n",
        "                axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig('pictures.png', bbox_inches='tight')\n",
        "    # Convert the saved image to Image\n",
        "    image = Image.open('pictures.png')\n",
        "    fig.canvas.draw()\n",
        "    image = Image.frombytes('RGB', fig.canvas.get_width_height(), fig.canvas.tostring_rgb())\n",
        "\n",
        "img_plot(model_)\n",
        "    # wandb.log({\"images\": [wandb.Image(image, caption=\"Test image prediction\")]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTSxk45VzBzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1GhO8Nx22gW1t-cL9NZnp_rNiIURNdbqj",
      "authorship_tag": "ABX9TyPMtDmBk985CbGP3/9oVVdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}